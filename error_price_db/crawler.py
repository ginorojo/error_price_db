import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from config import RATE_LIMIT_SECONDS, MAX_PRODUCTS
import time

def detect_pattern(url):
    """
    Detecta autom√°ticamente el patr√≥n de URL de producto
    seg√∫n el dominio del sitio.
    """
    if "sodimac.cl" in url:
        return "/product/"
    elif "falabella.com" in url:
        return "/product/"
    elif "lider.cl" in url:
        return "/producto/"
    else:
        return "/product/"

def crawl_site(start_url):
    """
    Recorre la categor√≠a agregada y devuelve URLs de productos
    detectando autom√°ticamente el patr√≥n seg√∫n el dominio.
    """
    pattern_contains = detect_pattern(start_url)
    visited = set()
    to_visit = [start_url]
    product_urls = set()

    print(f"üï∑Ô∏è Iniciando crawl en: {start_url}")
    print(f"üîé Buscando URLs que contengan: '{pattern_contains}'")

    while to_visit and len(product_urls) < MAX_PRODUCTS:
        url = to_visit.pop(0)
        if url in visited:
            continue
        visited.add(url)

        try:
            resp = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            if resp.status_code != 200:
                print(f"‚ö†Ô∏è Error {resp.status_code} al acceder a {url}")
                continue
            time.sleep(RATE_LIMIT_SECONDS)
        except Exception as e:
            print("‚ùå Error al acceder a:", url, e)
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        for a in soup.find_all("a", href=True):
            href = a['href']
            full_url = urljoin(start_url, href)

            # Mantenerse dentro del mismo dominio
            if not full_url.startswith(start_url.split('/')[0] + '//' + start_url.split('/')[2]):
                continue

            # Detectar productos
            if pattern_contains in full_url:
                url_clean = full_url.split('?')[0]
                if url_clean not in product_urls:
                    product_urls.add(url_clean)
                    print("üõí Producto detectado:", url_clean)
            else:
                if full_url not in visited and full_url not in to_visit:
                    to_visit.append(full_url)

        if len(visited) > 3000:
            print("‚õî M√°ximo de p√°ginas visitadas alcanzado, deteniendo crawl.")
            break

    print(f"‚úÖ Crawl terminado. Se detectaron {len(product_urls)} productos en {start_url}")
    return list(product_urls)
